---
title : "연구 노트"
use_math : true
layout : single
classes : wide
published : false
---

**1. 연구 노트**
- 2019/03/25
  - 주제 정리 : Its hard to learn 'optimal action' if it is hardly-explorable with high stochasticity
    - 에이전트는 어떤 행동에 대한 가치를 '분포'로써 estimate하고 있다. 
    이 때 어떤 reward가 굉장히 안 좋게 나왔고 따라서 분포 역시 negative하게 업데이트 된다면, 해당 reward가 정말로 분포적으로 봤을 때
    매우 흔하며 이 액션의 가치가 낮은지, 아니면 되게 높은 reward도 나오기 때문에 이 액션을 고수해야 하는지 학습해야 한다.
    - 즉 reward의 예측이 필요하다
    - 예측이 잘 될 수록 intrinsic reward 'curiosity'는 줄어들 것
  - 액션의 stochasticity, value, uncertainty(unawareness), count 등을 모두 고려한 exploration 방식
  - 어떤 액션이 굉장히 '결정적'이라면 (ex: stochastic reward/high variance/reward is quite different with previous) 
  반복이 필요
    - Curiosity about reward
    - Explore states that have not been seen before -> Explore rewards that are not been seen or uncertain
  - Reward-curious 네트워크의 아웃풋이 single reward의 예측이 아니라 분포라던지 확률이 되어야 함! 
    - 단순히 value distribution을 쓰는 것은 좋지 않음. 왜냐면 value distribution은 reward에 대한 정보보다 expected gain에 대한 정보이기 때문
    - Reward 자체가 stochastic한 경우에 추가적인 탐험을 통해 true value distribution을 확정해야
  - 이러한 접근이 neural-friendly하다는 점을 보여줘야 함
  - **PER 반드시 써야 함**
    - Priortize samples which is incorporating high reward curiousity
  - 내가 Bootstrapped DQN 까는 이유
    - Rely on uncertainty of parametric
    - So it's highly probabilistic
  - 고민
    - 실제로 벤치마크에서 reward가 stochastic한 경우가 많지 않은데 어떻게 적용시키지?
    - 내 알고리즘에서 distributional RL 쓰는 이유?
      - 1. True stochasticity를 catch-up 할 수 있다, bayesian 버전처럼
      - 2. Risk-sensitive policy를 써서 더 robust하게 statistically optimal action을 고수할 수 있다.
      - 3. 자체적으로 state-aliasing 기능이 있어서 parametric uncertainty를 더 잘 파악해 줄 거다?
  - 잡념
    - Explore라고 이름이 붙었지만 실제로는 그냥 exploit을 열심히 하는 논문들이 많은듯  
    
- 2019/03/29
  - Value Distribution의 variance를 단순히 줄이는 식으로 해버리면, 점진적으로 training 이후에 loss가 줄어든다 해도 intrinsic하게 줄일 수 없는 
  Boundary가 존재한다.  
  
- 2019/04/03
  - Variational inference로 unknown posterior distribution 추정하기. 
  - N이 커질 수록 estimate variance는 줄고, sample variance가 늘어날 수록 estimate variance는 커진다
    - BUT with toy example, as p is known as gaussian
  - PRML 10.2(Gaussian Mixture) 마저 보기
  - Nonparametric return distribution approximation for reinforcement learning
    - ICML 10년도  
- 2019/04/12
  - Central limit theorem을 활용하여 reward의 stochasticity를 확인하기
  - 하지만 table에서는 작용할텐데, 이를 neural net으로 어떻게 구현할까?
**2. 관련 논문**
- [Exploration By Uncertainty in reward space](https://openreview.net/forum?id=Sye2doC9tX)
  - Use the uncertainty in reward space as an intrinsic reward for policy exploration
    - 까이는 부분 1 : 'Intrinsic reward'는 원래 extrinsic reward와 상관 없는 term인데 함부로 바꿔 써버림
  - 까이는 부분 2 : 그냥 단순히 uncertainty in reward space가 크면 많이 explore한다?
    - Information gain 입장에서 전혀 좋지 않은 접근
    - Context에 맞는 조절이 필요(e.g., using gradual diminishing, see consistency of agents)
    - Should Find Optimal Condition for explore&high reward uncertainty!
  - 까이는 부분 3 : Reward를 predicting하는게 아니라 TD error (value)를 predicting함
    - 이건 reward uncertainty가 아니라 value uncertainty
    - 그냥 예상보다 value가 더 높게 나왔으면(물론 reward 때문일 수도 있지만) greedy하게 ㄱㄱ
  - 알고리즘 1에서 **만약 수식상 r이 매우 stochastic하다면 L은 절대 줄어들 수 없다.**
  - **그냥 TD error가 큰 쪽으로 explore하는 건 내 task에서 오히려 exploit하게 나올 것
    - 이건 explore가 아니라 그냥 초반에 더 유리하다고 보이는 액션을 꾸준히 exploit하는 것  
- [Belief Reward Shaping RL](https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16912)
  - 그닥...
  - Reward에 대한 prior belief가 강하게 작용할 수록 value update가 느리게 일어난다.
- 
    
**3. 읽을 논문**
- [Near-Bayesian Exploration in Polynomial Time](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.5715&rep=rep1&type=pdf)
  - Uncertainty-driven search in state-space
  - After executing several number of actions
- TRPO
  - Trust region 정의
- [#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning](https://arxiv.org/abs/1611.04717)
  - Count-based 방식 리뷰
- [VIME](https://arxiv.org/abs/1605.09674)
- [Reward shaping](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)
  - Andrew Ng
- [Reward Estimation for Variance Reduction in Deep Reinforcement Learning](https://arxiv.org/abs/1805.03359)
- https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005062
- [Navigating with grid-like representations in artificial agents](https://deepmind.com/blog/grid-cells/)
- [Deep Successor Reinforcement Learning](https://arxiv.org/pdf/1606.02396.pdf)
  - Hippocampus 모사, reward term 분리
- EPISODIC CURIOSITY THROUGH REACHABILITY (ICLR 2019, Deepmind and Zurich)
