---
title: "RL Note"
layout: single
classes: wide
use_math: true
---

**0. 'Markov' Things**  
- Markov chain : Stochastic process in which state transition Pr is determined by the previous state only
  - <S, P>
  - P는 arbitrary state s에서 가능한 s'으로의 transition probability를 저장하고 있음
- Markov reward process : Markov chain with values
  - <S, P, R, $$\gamma$$>
- Markov decision process : Markov reward process with decisions(or actions)
  - <S, A, P, R, $$\gamma$$>
- Partially Observable Markov Decision PRocess : MDP with hidden states (HMM)
  - <S, A, O, P, R, Z, $$\gamma$$>
  - O is a set of partial observations
  

**1. Goal of RL**
- return $$G_t$$를 최대화하는 것
  -$$G_t = R_{t+1}+{\gamma}R_{t+2}+{\gamma}R_{t+3}...$$
  - 이때 discount term은 cyclic MDP혹은 매우 긴 에피소드에서 infinite return을 막기 위함도 있으나, Uncertatinty about the future을 반영하는 목적이 더 크다.
- Stochastic한 process인 만큼, return의 기대값을 표현해야 함
  - In given state s, $$v(s) = E[G_t|s] = E[R_{t+1}+{\gamma}R_{t+2}+{\gamma}R_{t+3}...|s] = E[R_{t+1}+{\gamma}v(S_{t+1})|s]$$
  - Or for MDP, action-value function $$q(s,a) = E[G_t|s,a]$$
  
**2. Bellman Equation**
- 위의 expected return을 풀어쓴 식에서 v(s) (value function)은 immediate reward와 discounted value의 합으로 표현됨
  - 이것이 Bellman Equation의 시작
- 위 식을 matrix을 활용한 closed-form 형식으로 작성하면 $$v=R+{\gamma}Pv$$이며 따라서 행렬 계산을 통해 value function을 직접 구할 수 있음
  - 그러나 computational complexity가 너무 높기 때문에 (역행렬 계산 + 행렬곱), direct solution을 구하기 어려움
  - 이를 보완하기 위해 다양한 iterative algorithm이 존재함
- 결국 우리의 목표는 어떤 policy를 선택해서 expected reward를 많이 확보할지 결정해야
  - 모델의 P, R을 고려해서 Given Stochastic Policy를 평가하는 방법이 바로 Bellman Expectation Equation
  - $$V_{\pi}(s) = \sum_a{\pi}(a|s)q_{\pi}(s,a)$$
  - $$q_{\pi}(s,a) = R_a+{\gamma}\sum_{s'}P_{s'}^aV_{\pi}(s')$$
  - 이를 합치면 $$V_{\pi}(s) = \sum_a{\pi}(a|s)(R_a+\sum_{s'}P_{s'}^aV_{\pi}(s'))$$
  - Action value 버전도 동일하게 존재
  - MDP에서 가능한 차기 action의 reward, 그리고 차기 state의 value를 합쳤기 때문에 expectation equation이라고 부름
- 이렇게 어떤 given policy를 평가하고, 앞으로 볼 policy iteration처럼 평가를 기반으로 policy를 개선하는 방식도 쓸 수 있으나,
  - 어차피 필요한 것은 결국 optimal policy이기 때문에 over all policy에서 가장 높은 state 혹은 action value을 얻는 Bellman Optimality Equation도 사용 가능
  - $$V_{*}(s) = max_{a}q_{*}(s,a)$$, 이때 *는 optimal policy(구했거나, 혹은 구해나가야 할)
  - $$q_{*}(s,a) = R_a+{\gamma}\sum_{s'}P_{s'}^aV_{*}(s')$$
  - 이를 합치면 $$V_{*}(s) = max_{a}R_a+{\gamma}\sum_{s'}P_{s'}^aV_{*}(s')$$
  - Action value 버전도 동일하게 존재
- Bellman Expectation Equation은 행렬을 활용한 linear equation으로 표현 가능했지만, Optimality equation은 max operator로 인해 불가능
  - 따라서 애초에 closed form solution은 구할 수 없고, 임의로 가정한 optimal policy에서 시작해 iterative하게 이를 수렴 시키는 알고리즘이 추후 활용됨
  - Transition matrix(model)을 제대로 몰라도 활용할 수 있으며 optimal policy로 이론적으로 수렴하기에 model-free RL의 기초가 되는 식
  - B.E는 model에 대한 oracle이 있는 상황에서 주어진 policy를 개선할 수 있도록 함, B.O는 주어진 policy가 아닌 optimal policy를 바로 찾도록 함
  - 이는 추후 sampling을 할 때 SARSA와 Q-learning의 차이가 되기도 함
  - 또한 다음절의 DP에서는 policy iteration과 value iteration의 차이가 되기도 함
    
**3.Iterative Methods for policy control**  

**3.1 Dynamic Programming (Full-width backup)**  
- DP는 주어진 문제를 overlapping subproblems으로 나눠 각 subproblem의 optimal answer를 계산, 캐싱(back-up)하고 이를 통해 전체 problem의 optimal answer를 구하는 알고리즘
  - 이를 MDP에도 적용 가능
  - 단 MDP의 model을 알고 있어 subproblem을 나눌 수 있다는 가정이 필요함
- DP를 쓰는 목적에 따라 prediction or control로 나눌 수 있음
  - prediction : Input policy에 대한 value function $$v_\pi$$를 평가함
  - control : 평가를 넘어서 바로 optimal policy {\pi}_*를 찾아냄
  - 두 단계 모두 주어진 모델에 대해 완전히 이해하고 있다는 가정이 필요하다 (Planning)
    - 만약 이러한 가정이 없다면 planning이 아닌 learning
  - 주로 control에 대해서만 다루겠음 (control = prediction + improvement이므로)
    - D.Silver 교수님 강의에서는 'iterative' control을 iteration으로 부르는 것 같음. 즉 DP에서는 Iteration이 핵심
- Policy Iteration
  - Explicit Policy에 대한 value function evaluation + greedy improvement
  - Given state s에서 Greedy improvement를 통해 새로 결정된 policy에서 결정한 action value는, 기존 policy에서 결정한 action value보다 최소한 같거나 크다.
  - Bellman expectation equation
- Value Iteration
  - Optimal Value function iterative evaluation + 최종 policy improvement
  - Policy is NOT explicit but implicit
  - Intermediate value functions may not correspond to any policy (and not converged)
  - Bellman optimality equation
- 두 iteration에 state-value function, action-value function 모두 적용 가능하지만 보통 action value 버전을 많이 사용
  - Greedy improvement을 기반으로 한 action 선택 때문
  - 추후 Monte-carlo에서 설명
- Policy Iteration, Value Iteration 모두 주어진 policy가 explicit하든 아니든, 하나의 fixed value function $$v*$$로 수렴한다.
  - 이는 Iterative value function approximator $$v_{k+1} = R+{\gamma}Pv_k$$가 $$\gamma-contraction$$하여 contraction mapping theorem을 사용 가능하기 때문이다.
    
**3.2 Monte-carlo (Sample backup)**  
- Monte-Carlo : using empirical mean return
  - 방문한 s에 대해서 첫 time-step 혹은 every time-step에 대해 incremental average of return을 구함
  - 이를 expected return의 empirical sample로 활용
  - 이때 수식적으로 이를 value의 update로 표현 가능
  - $$V(s_t) = V(s_t) + {\alpha}(G_t - V(s_t))$$
- 그러나 State value function을 그대로 사용할 경우 문제가 생기는데, policy를 평가한 후 improve하는 과정에서 model의 transition probability를 모르기 때문에 greedy action을 선택할 수 없다.
  - 따라서 policy improvement에 있어 $$P$$를 몰라도 되는 action value function $$Q$$을 이용해 iteration을 진행한다.
- 또한 Control 과정에서 기존 DP의 iteration처럼 모든 state 혹은 action pair에 대해서 update하여 fully true policy value를 얻는게 아니라, sampling을 활용하여 하나의 에피소드가 끝나면 해당 에피소드에서 거친 (s,a)에 대해서만 우선 즉각적으로 update한다.
  - 이를 통해 에이전트는 에피소드가 끝날 때마다 항상 most recent action value function을 활용하여 새로운 improved policy를 얻을 수 있게 된다.
  ![image](https://user-images.githubusercontent.com/46081019/52908335-2b65b300-32b7-11e9-8119-8b849749e8bb.png)
    
**3.3 Temporal difference (Sample backup)**  
- TD는 bootstrapping을 통해 error가 나더라도, on-line으로 학습할 수 있도록 monte-carlo를 개선함
  - TD works in continuing, incomplete env
- 위 식의 $$G_t$$를 TD target인 $$R_{t+1}+{\gamma}V(s_{t+1})$$로 대체
- 이때 당연히 아직 V는 true policy value가 아니기 때문에 bias가 발생
  - 그러나 이를 통해 variance는 줄일 수 있음
  - Multiple step, action에 의해 결정되는 empirical return의 경우 variance가 높을 수 있다.
  - TD target은 오직 하나의 transition에만 의존함
- 언급한 이유로 인해 control에서도 policy evaluation 과정에서 3.2 MC 대신 TD를 많이 사용함. Update for every time step
  ![image](https://user-images.githubusercontent.com/46081019/52908453-754f9880-32b9-11e9-9859-5c2b131936c9.png)
  - Policy prediction과 improvements(e-greedy)가 사용되고 있는 슈도 알고리즘
    
**3.4 Comparing MC and TD**
- MC와 TD는 모두 full-width backup이 아닌 sample backup 방식
  - DP처럼 이전 iter의 모든 state 정보를 전부 캐싱하고 사용하는게 아니라 한 에피소드 혹은 한 스텝에 대해서만 캐싱
  - 따라서 전체 MDP에 대한 model을 갖고 있지 않아도 learning 방식을 통해 iteration을 진행할 수 있음
  - *Model-free*
- MC는 bootstrapping하지 않지만 TD는 bootstrapping 사용
  - Monte-carlo는 한 episode에 대해 끝까지 return을 확인하면서 expected reward에 대한 unbiased sample을 얻어냄
  - 이를 바로 learning에 활용하기 때문에 bootstrapping이 아님. 보다 정확하고 robust
  - 하지만 반드시 에피소드가 끝나야 한다는 단점 존재
- Monte-carlo는 visited state에 대한 non-bootstrapping experience를 활용하기 때문에, 특정 경험에 지나치게 의존적일 수 있다.
  - 또한 Monte-carlo는 여러 step에 dependent하기 때문에 Markov process의 특성과도 잘 맞지 않는다.
  ![image](https://user-images.githubusercontent.com/46081019/52907629-965bbd80-32a8-11e9-89d1-0d2f90c720db.png)  
- Monte-carlo의 unbiased estimate와 TD의 특성을 합쳐 n-step Return을 estimate하기도 함
- $$G_t^{n} = R_{t+1}+{\gamma}R_{t+2}+...+{\gamma}^{n-1}R_{t+n}+{\gamma}^{n}V(S_{t+n})$$
- 적절한 n은 hyper-parameter로서 조정됨
- 또한 1-step부터 n-step까지의 return을 합치기도 함  
  - Forward-view : weighted average방식. step이 길어질 수록 ratio $$\lambda$$로 decaying시킨다. $$\lambda$$가 0일 경우 TD와 동일.
    - $$\lambda$$의 크기에 따라 MC와 TD의 중간에서 왔다갔다함
  - Backward-view : Eligibility Traces 방식. 방문할때마다 state의 eligibility trace를 갱신하고, TD error를 eligibility trace에 비례하게 전파함
    - Forward view는 fixed $$\lambda$$를 여러 state에 동시 적용해버리는 반면, eligibility trace는 state의 frequency와 recency를 고려함
    
**4. Off-policy learning**  
- Off-policy : 배우려는 타깃 policy와 현재 policy가 다름
  - Learn about policy $$\pi$$ from *experience sampled from* $$\mu$$
  - 따라서 예전 policy에서의 경험 시퀀스를 바탕으로 새로운 policy에 대해 학습할 수 있다. Re-use experience generated from old policies
  - Human data (or policy)로부터 imitation learning 등을 시도해 볼 수 있다.
  - 기존의 exploratory policy는 유지하면서 optimal policy도 학습을 시도할 수 있다.
  - 즉 하나의 behavior policy를 기반, 다양한 policy를 학습 시도할 수 있다.
  - **Q-learning은 결국 Bellman optimality equation의 sampling 버전이다**
