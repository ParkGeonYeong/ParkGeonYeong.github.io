---
title: "RL Note"
layout: single
classes: wide
---

**0. 'Markov' Things**  
- Markov chain : Stochastic process in which state transition Pr is determined by the previous state only
  - <S, P>
  - P는 arbitrary state s에서 가능한 s'으로의 transition probability를 저장하고 있음
- Markov reward process : Markov chain with values
  - <S, P, R, %%\gamma%%>
- Markov decision process : Markov reward process with decisions(or actions)
  - <S, A, P, R, %%\gamma%%>
- Partially Observable Markov Decision PRocess : MDP with hidden states (HMM)
  - <S, A, O, P, R, Z, $$\gamma$$>
  - O is a set of partial observations
  

**1. Goal of RL**
- return $$G_t$$를 최대화하는 것
  - $$G_t = R_{t+1}+{\gamma}R_{t+2}+{\gamma}R_{t+3}...
  - 이때 discount term은 cyclic MDP혹은 매우 긴 에피소드에서 infinite return을 막기 위함도 있으나, Uncertatinty about the future을 반영하는 목적이 더 크다.
- Stochastic한 process인 만큼, return의 기대값을 표현해야 함
  - In given state s, $$v(s) = E[G_t|s] = E[R_{t+1}+{\gamma}R_{t+2}+{\gamma}R_{t+3}...|s] = E[R_{t+1}+{\gamma}v(S_{t+1})|s]$$
  - Or for MDP, action-value function $$q(s,a) = E[G_t|s,a]$$
  
**2. Bellman Equation**
- 위의 expected return을 풀어쓴 식에서 v(s) (value function)은 immediate reward와 discounted value의 합으로 표현됨
  - 이것이 Bellman Equation의 시작
- 위 식을 matrix을 활용한 closed-form 형식으로 작성하면 $$v=R+{\gamma}Pv$$이며 따라서 행렬 계산을 통해 value function을 직접 구할 수 있음
  - 그러나 computational complexity가 너무 높기 때문에 (역행렬 계산 + 행렬곱), direct solution을 구하기 어려움
  - 이를 보완하기 위해 다양한 iterative algorithm이 존재함
- 결국 우리의 목표는 어떤 policy를 선택해서 expected reward를 많이 확보할지 결정해야
  - 모델의 P, R을 고려해서 Given Stochastic Policy를 평가하는 방법이 바로 Bellman Expectation Equation
  - $$V_{\pi}(s) = \sum_a{\pi}(a|s)q_{\pi}(s,a)$$
  - $$q_{\pi}(s,a) = R_a+{\gamma}\sum_{s'}P_{s'}^aV_{\pi}(s')$$
  - 이를 합치면 $$V_{\pi}(s) = \sum_a{\pi}(a|s)(R_a+\sum_{s'}P_{s'}^aV_{\pi}(s'))$$
  - Action value 버전도 동일하게 존재
  - MDP에서 가능한 차기 action의 reward, 그리고 차기 state의 value를 합쳤기 때문에 expectation equation이라고 부름
- 이렇게 어떤 given policy를 평가하고, 앞으로 볼 policy iteration처럼 평가를 기반으로 policy를 개선하는 방식도 쓸 수 있으나,
  - 어차피 필요한 것은 결국 optimal policy이기 때문에 over all policy에서 가장 높은 state 혹은 action value을 얻는 Bellman Optimality Equation도 사용 가능
  - $$V_{*}(s) = max_{a}q_{*}(s,a)$$, 이때 *는 optimal policy(구했거나, 혹은 구해나가야 할)
  - $$q_{*}(s,a) = R_a+{\gamma}\sum_{s'}P_{s'}^aV_{*}(s')$$
  - 이를 합치면 $$V_{*}(s) = max_{a}R_a+{\gamma}\sum_{s'}P_{s'}^aV_{*}(s')$$
  - Action value 버전도 동일하게 존재
- Bellman Expectation Equation은 행렬을 활용한 linear equation으로 표현 가능했지만, Optimality equation은 max operator로 인해 불가능
  - 따라서 애초에 closed form solution은 구할 수 없고, 임의로 가정한 optimal policy에서 시작해 iterative하게 이를 수렴 시키는 알고리즘이 추후 활용됨
  - Transition matrix(model)을 제대로 몰라도 활용할 수 있으며 optimal policy로 이론적으로 수렴하기에 model-free RL의 기초가 되는 식
  - B.E는 model에 대한 oracle이 있는 상황에서 주어진 policy를 개선할 수 있도록 함, B.O는 주어진 policy가 아닌 optimal policy를 바로 찾도록 함
  - 이는 추후 sampling을 할 때 SARSA와 Q-learning의 차이가 되기도 함
**3.Iterative Methods**  
**3.1 Dynamic Programming**  
- DP는 주어진 문제를 overlapping subproblems으로 나눠 각 subproblem의 optimal answer를 구하고 이를 통해 전체 problem의 optimal answer를 구하는 알고리즘
  - 이를 MDP에도 적용 가능
  - 단 MDP의 model을 알고 있어 subproblem을 나눌 수 있다는 가정이 필요함
- DP를 쓰는 목적에 따라 prediction or control로 나눌 수 있음
  - prediction : Input policy에 대한 value function $$v_\pi$$를 평가함
  - control : 평가를 넘어서 바로 optimal policy {\pi}_*를 찾아냄
- 
  

  
