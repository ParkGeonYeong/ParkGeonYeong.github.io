---
title: "Dropout 몇 줄 정리"
use_math: True
layout: single
classes: wide
---

**0. Motivation**  
- ML에서는 항상 다양한 네트워크를 앙상블링 하는게 성능 향상에 도움됨
- 그러나 이는 computational load가 너무 높고, explicit하게 test에 적용할 때도 계속 multi-network를 돌려야 한다는 단점이 있음
- 이를 보완해, multiple thinned networks의 효과를 근사하여 확률론적으로 몇몇 뉴런을 비활성화
- 이는 곧 extensive하게 weight를 sharing하는 multi-network 효과를 봄
- Test 시엔 다시 sampled weights의 unbiased estimation을 고려해서 pW로 가중치를 대체
- 이를 통해 크게 두 가지 문제를 해결할 수 있음
- 첫째는 co-adaptation의 해결. 이는 곧 특정 latent variable에 성능이 좋은 neuron population민을 편협적으로 모아서 적응시키는걸 방지. 
- 개별 뉴런이 상황에 따라 어떤 뉴런 집합과 연결될 지 랜덤하게 결정하여 일반적인 구성에서 성능이 좋도록 fitting
- 이를 통해 overfitting 문제 해결
- 또 한 가지는 자체 regularization 효과를 볼 수 있음.
- 위와 비슷한 맥락으로, dropout은 마치 network에 noise를 더하는 과정과 비슷하기 때문에 regularizing 효과를 볼 수 있음
- 논문에서는 몇 가지 다른 방식과 함께 training을 시켰는데, learning-rate decay, gradient clipping, unsupervised pretraining이 그 것
- 그 중 자기 자신을 예측하도록 pre-training하는 unsupervised는 pre-training 이후 weight를 (1/p)배 하여, 이후 과정에서 bias가 생기지 않도록 함
